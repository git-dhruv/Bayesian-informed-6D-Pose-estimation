{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  \n",
    "sys.path.append('../src')  \n",
    "\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# script_directory = os.path.dirname(os.path.realpath(__file__))\n",
    "# project_root = os.path.join(script_directory, '..')\n",
    "# sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthB.png\", cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(dpt)\n",
    "plt.figure()\n",
    "dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthA.png\", cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(dpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ruinDepth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 3, kernel_size=(3,3), padding=1)\n",
    "        self.rl1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "\n",
    "        self.cn2 = nn.Conv2d(3, 7, kernel_size=(3,3), padding=1)\n",
    "        self.rl2 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(7)\n",
    "\n",
    "        self.cn3 = nn.Conv2d(7, 14, kernel_size=(3,3), padding=1)\n",
    "        self.rl3 = nn.LeakyReLU()\n",
    "        \n",
    "        self.tcn1 = nn.ConvTranspose2d(14, 7, kernel_size=3, padding=1)\n",
    "        self.rl4 = nn.Tanh()\n",
    "        self.bn4 = nn.BatchNorm2d(7)\n",
    "        self.bn4_2 = nn.BatchNorm2d(7)\n",
    "\n",
    "        self.tcn2 = nn.ConvTranspose2d(7, 3, kernel_size=3, stride=4, output_padding=1, padding=1)\n",
    "        self.rl5 = nn.Tanh()\n",
    "\n",
    "        self.tcn3 = nn.ConvTranspose2d(3, 2, kernel_size=3)\n",
    "        self.rl6 = nn.ReLU()\n",
    "\n",
    "        self.mpool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mpool(self.bn1(self.rl1(self.cn1(x))))\n",
    "        x2 = self.mpool(self.bn2(self.rl2(self.cn2(x))))\n",
    "        x = (self.rl3(self.cn3(x2)))\n",
    "\n",
    "        x = self.bn4(self.bn4_2(self.rl4(self.tcn1(x))) + x2)\n",
    "        self.x = x\n",
    "        x = (self.rl5(self.tcn2(x))) \n",
    "        out = self.rl6(self.tcn3(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from src.processDepth import depthCompletion\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        root = r'C:\\Users\\dhruv\\Desktop\\680Final\\data\\0050'\n",
    "        self.depthFiles =  sorted(glob.glob(os.path.join(root, 'depth/*.png')))\n",
    "        self.depthFillFiles = sorted(glob.glob(os.path.join(root, 'depth/*.png')))\n",
    "\n",
    "        self.depthtransform = depthCompletion(15, None, None, None, None)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.depthFiles)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        img = np.float64(cv2.imread(self.depthFillFiles[idx], cv2.IMREAD_UNCHANGED))\n",
    "        goodDepth = self.depthtransform.fillDepth(img)\n",
    "        dp = transforms.ToTensor()(goodDepth)/1e3\n",
    "        tar = transforms.ToTensor()(img)/1e3\n",
    "        tar[tar>0.01] = 1\n",
    "        tar[tar<=0.01] = 0\n",
    "        return [dp, tar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torchvision\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "class train(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        loader = SimDataset()\n",
    "        self.train_loader = torch.utils.data.DataLoader(loader, shuffle=True, batch_size=32)\n",
    "        self.network = ruinDepth()\n",
    "        # self.tLoss = nn.MSELoss()\n",
    "        self.CELoss = nn.CrossEntropyLoss(weight=torch.Tensor([0.6,0.4]))\n",
    "\n",
    "        self.loss = 0\n",
    "        self.itr = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        pred = self.network.forward(images.float())        \n",
    "        # mask = targets==0\n",
    "        # loss =  0.5*self.tLoss(pred, targets.float()) + self.tLoss(pred[mask], targets[mask].float()) \n",
    "        loss = self.CELoss(pred, targets[:,0,:,:].long())\n",
    "\n",
    "        if batch_idx==0:\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            if self.current_epoch%10==0:\n",
    "                imgs = [i.unsqueeze(0).cpu().detach() for i in self.network.x[0]]\n",
    "                grid_img = utils.make_grid(imgs)  \n",
    "                self.logger.experiment.add_image('Inference/Skip Layer', grid_img, self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Input', images[0], self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Output', pred[0].unsqueeze(0), self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Target', targets[0], self.current_epoch)\n",
    "\n",
    "\n",
    "        self.loss+=loss; self.itr+=len(images)\n",
    "        return {'loss': loss}\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\n",
    "        self.log(\"loss\", self.loss/self.itr)\n",
    "        self.loss = 0\n",
    "        self.itr = 0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# TensorBoard Logger\n",
    "logger = pl.loggers.TensorBoardLogger('../logs/tb_logs', name='depthruiner')\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='epoch',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='my_model-{epoch:02d}-{loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Learning Rate Monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "model = train()\n",
    "\n",
    "# Trainer with logger, checkpoint, and LR monitor\n",
    "trainer = pl.Trainer(\n",
    "    devices=1, \n",
    "    accelerator=\"gpu\", \n",
    "    max_epochs=100, \n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, lr_monitor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# trainer = pl.Trainer(\n",
    "#     devices=1, \n",
    "#     accelerator=\"gpu\", \n",
    "#     max_epochs=100, \n",
    "#     logger=logger,\n",
    "#     callbacks=[checkpoint_callback, lr_monitor]\n",
    "# )\n",
    "\n",
    "model = model.load_from_checkpoint(r'C:\\Users\\dhruv\\Desktop\\680Final\\notebooks\\checkpoints\\my_model-epoch=41-loss=0.01.ckpt')\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotimg(img):\n",
    "#     plt.figure(); plt.imshow(img);plt.show()\n",
    "    \n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "# model.network.eval()\n",
    "# for img, tar in train_loader:\n",
    "#     synth = img[0][0].cpu().detach().numpy(); plotimg(synth)\n",
    "#     target = tar[0][0].cpu().detach().numpy(); plotimg(target)\n",
    "#     pred = model.network.forward(img.cuda()); pred = torch.argmax(pred, dim=1); pred = pred[0].cpu().detach().numpy()\n",
    "#     plotimg(pred)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthCompletion2:\n",
    "    \"\"\"\n",
    "    Unsuperivsed Depth completion\n",
    "    Reference: \n",
    "        J. Ku, et al., \"In Defense of Classical Image Processing: Fast Depth Completion on the CPU,\" 2018.\n",
    "    \"\"\"    \n",
    "    def __init__(self, maxDepth, custom_kernel, full_5, full_7, full_31):\n",
    "        self.max_depth = maxDepth\n",
    "        if custom_kernel is None:\n",
    "            #Replace this garbage with a function later\n",
    "            custom_kernel = np.asarray([[0, 0, 0, 1, 0, 0, 0],\n",
    "                                        [0, 0, 1, 1, 1, 0, 0],\n",
    "                                        [0, 1, 1, 1, 1, 1, 0],\n",
    "                                        [1, 1, 1, 1, 1, 1, 1],\n",
    "                                        [0, 1, 1, 1, 1, 1, 0],\n",
    "                                        [0, 0, 1, 1, 1, 0, 0],\n",
    "                                        [0, 0, 0, 1, 0, 0, 0],], dtype=np.uint8)\n",
    "\n",
    "        if full_5 is None:\n",
    "            full_5 = np.ones((5, 5), np.uint8)\n",
    "        if full_7 is None:\n",
    "            full_7 = np.ones((7, 7), np.uint8)\n",
    "        if full_31 is None:\n",
    "            full_31 = np.ones((31, 31), np.uint8)\n",
    "\n",
    "        self.custom_kernel = custom_kernel\n",
    "        self.full_kernel_5 = full_5\n",
    "        self.full_kernel_7 = full_7\n",
    "        self.full_kernel_31 = full_31\n",
    "\n",
    "    def fillDepth(self,inputImage):\n",
    "        #Convert the mm to meters to be consistent with the paper\n",
    "        img = np.float32(inputImage.copy())/1e3\n",
    "\n",
    "        self.max_depth = img.max()*1.2\n",
    "        #Threshold to maxDepth\n",
    "        # img[img>self.max_depth] = self.max_depth\n",
    "\n",
    "        #Valid image mask - 10 cm away\n",
    "        valid_depth_mask = img>1e-3\n",
    "\n",
    "        #1. Invert the Depth\n",
    "        img[valid_depth_mask] = self.max_depth - img[valid_depth_mask]\n",
    "\n",
    "        #2. Custom Kernel - ruins performance on inference :)\n",
    "        # img = cv2.dilate(img, self.custom_kernel)\n",
    "\n",
    "        #3. Small Hole Closure\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, self.full_kernel_5)\n",
    "        \n",
    "        #4. Small Hole Fill\n",
    "        invalid_mask = img<1e-3 #Remember depth is not changed in inversion\n",
    "        img[invalid_mask] = cv2.dilate(img, self.full_kernel_7)[invalid_mask]        \n",
    "        \n",
    "        #5. Large Hole Fill (authors loose patience at this step tbh)\n",
    "        invalid_mask = img<1e-3\n",
    "        img[invalid_mask] = cv2.dilate(img, self.full_kernel_31)[invalid_mask]\n",
    "\n",
    "        # #6. Median+Bilateral\n",
    "        img = cv2.medianBlur(img, 5)\n",
    "        img = cv2.bilateralFilter(img, 5, 1.5, 2.0) #Slow but better\n",
    "\n",
    "        #8 Depth correction\n",
    "        valid_depth_mask = img>1e-3\n",
    "        img[valid_depth_mask] = self.max_depth - img[valid_depth_mask]\n",
    "\n",
    "        return img*1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthB.png\", cv2.IMREAD_UNCHANGED)\n",
    "# dpt = np.float64(dpt)\n",
    "# dpt = transforms.ToTensor()(dpt)/1e3\n",
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "\n",
    "\n",
    "\n",
    "# # img2 = img.clone()[0][0]\n",
    "# img2 = dpt.clone()[0]\n",
    "# img2[mask] = img2.min()\n",
    "# plotimg(img2)\n",
    "# print(img2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.network.eval()\n",
    "# # with torch.no_grad():\n",
    "# #     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # # plotimg(pred_)\n",
    "# # # pred_ = pred.copy()\n",
    "# # dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# # mask = dp==0\n",
    "\n",
    "# from celluloid import Camera\n",
    "# fig, axs = plt.subplots(1, 2)\n",
    "# camera = Camera(fig)\n",
    "\n",
    "# root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "# depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "# model.network.eval()\n",
    "# # train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "# # model.network.eval()\n",
    "# # for img, tar in train_loader:\n",
    "# for i in depthFiles:\n",
    "#     dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "#     dpt = np.float64(dpt)\n",
    "#     dpt = transforms.ToTensor()(dpt)/1e3\n",
    "#     synth = dpt[0].cpu().detach().numpy().copy()\n",
    "#     # target = tar[0][0].cpu().detach().numpy();# plotimg(target)\n",
    "#     with torch.no_grad():\n",
    "#         pred_ = torch.argmax(model.network.forward(dpt.cuda().unsqueeze(0).float()),dim=1)[0].cpu().detach().numpy()\n",
    "#     dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((5,5))).fillDepth(pred_)\n",
    "#     mask = dp==0\n",
    "#     synth[mask] = synth.min()\n",
    "#     print(synth.min(),synth.max())\n",
    "#     axs[0].imshow(dpt[0].cpu().detach().numpy() * 1e3, cmap='gray')\n",
    "#     axs[0].axis('off')\n",
    "#     axs[0].set_title('Raw Image')\n",
    "\n",
    "#     axs[1].imshow(synth * 1e3, cmap='gray')\n",
    "#     axs[1].axis('off')\n",
    "#     axs[1].set_title('Predicted Image')\n",
    "\n",
    "#     # plt.show()\n",
    "#     camera.snap()\n",
    "# animation = camera.animate()\n",
    "# animation.save('animation.mp4', fps=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "\n",
    "from celluloid import Camera\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "camera = Camera(fig)\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "model.network.eval()\n",
    "train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "from tqdm import tqdm\n",
    "for img, tar in tqdm(train_loader):\n",
    "    synth = img[0][0].cpu().detach().numpy().copy()\n",
    "    target = tar[0][0].cpu().detach().numpy();# plotimg(target)\n",
    "    with torch.no_grad():\n",
    "        pred_ = torch.argmax(model.network.forward(img.cuda().float()),dim=1)[0].cpu().detach().numpy()\n",
    "    dp = depthCompletion2(2, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,3))).fillDepth(pred_)\n",
    "    mask = dp==0\n",
    "    axs[0].imshow(synth)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Synthetic')\n",
    "\n",
    "    synth[mask] = 0\n",
    "    \n",
    "\n",
    "    axs[1].imshow(synth * 1e3)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Simulated')\n",
    "    synth = img[0][0].cpu().detach().numpy().copy()\n",
    "    mask = target==0\n",
    "    synth[mask] = 0\n",
    "\n",
    "\n",
    "    axs[2].imshow(synth)\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title('Real')\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "    camera.snap()\n",
    "    # break\n",
    "animation = camera.animate()\n",
    "animation.save('animation.mp4', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2; import glob; import os\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "camera = Camera(fig)\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "rgbFiles =  sorted(glob.glob(os.path.join(root, '*rgbB.png')))[:30]\n",
    "\n",
    "for i,j in zip(depthFiles, rgbFiles):\n",
    "    dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "    # dpt = np.float64(dpt)\n",
    "    # dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)    \n",
    "    axs[0].imshow(dpt, cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Depth')\n",
    "    synth = cv2.imread(j, cv2.IMREAD_COLOR)\n",
    "\n",
    "    axs[1].imshow(synth)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('RGB')\n",
    "\n",
    "    # plt.show()\n",
    "    camera.snap()\n",
    "animation = camera.animate()\n",
    "animation.save('dataset.mp4', fps=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "import glob, os, tqdm\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\train_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))\n",
    "model.network.eval()\n",
    "for i in tqdm.tqdm(depthFiles):\n",
    "    dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "    dpt = np.float64(dpt)\n",
    "    dpt = transforms.ToTensor()(dpt)/1e3\n",
    "    synth = dpt[0].cpu().detach().numpy().copy()\n",
    "    with torch.no_grad():\n",
    "        pred_ = torch.argmax(model.network.forward(dpt.cuda().unsqueeze(0).float()),dim=1)[0].cpu().detach().numpy()\n",
    "    dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((5,5))).fillDepth(pred_)\n",
    "    mask = dp==0\n",
    "    synth[mask] = 0\n",
    "    synth = synth * 1e3\n",
    "    synth = synth.astype(np.uint16)\n",
    "    file = i.split(\".png\")[0]+\"_fake.png\"\n",
    "    cv2.imwrite( file, synth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.float64(cv2.imread(i, cv2.IMREAD_UNCHANGED))\n",
    "d2 = np.float64(cv2.imread(file, cv2.IMREAD_UNCHANGED))\n",
    "print(d1.min())\n",
    "plt.imshow(d1)\n",
    "plt.figure()\n",
    "d2[d2==0] = 625\n",
    "plt.imshow(d2)\n",
    "print(file)\n",
    "# plt.imshow(d2!=d1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
