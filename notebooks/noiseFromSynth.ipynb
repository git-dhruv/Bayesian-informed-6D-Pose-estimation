{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  \n",
    "sys.path.append('../src')  \n",
    "\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# script_directory = os.path.dirname(os.path.realpath(__file__))\n",
    "# project_root = os.path.join(script_directory, '..')\n",
    "# sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthB.png\", cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(dpt)\n",
    "plt.figure()\n",
    "dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthA.png\", cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(dpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ruinDepth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 3, kernel_size=(3,3), padding=1)\n",
    "        self.rl1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "\n",
    "        self.cn2 = nn.Conv2d(3, 7, kernel_size=(3,3), padding=1)\n",
    "        self.rl2 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(7)\n",
    "\n",
    "        self.cn3 = nn.Conv2d(7, 14, kernel_size=(3,3), padding=1)\n",
    "        self.rl3 = nn.LeakyReLU()\n",
    "        \n",
    "        self.tcn1 = nn.ConvTranspose2d(14, 7, kernel_size=3, padding=1)\n",
    "        self.rl4 = nn.Tanh()\n",
    "        self.bn4 = nn.BatchNorm2d(7)\n",
    "        self.bn4_2 = nn.BatchNorm2d(7)\n",
    "\n",
    "        self.tcn2 = nn.ConvTranspose2d(7, 3, kernel_size=3, stride=4, output_padding=1, padding=1)\n",
    "        self.rl5 = nn.Tanh()\n",
    "\n",
    "        self.tcn3 = nn.ConvTranspose2d(3, 2, kernel_size=3)\n",
    "        self.rl6 = nn.ReLU()\n",
    "\n",
    "        self.mpool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mpool(self.bn1(self.rl1(self.cn1(x))))\n",
    "        x2 = self.mpool(self.bn2(self.rl2(self.cn2(x))))\n",
    "        x = (self.rl3(self.cn3(x2)))\n",
    "\n",
    "        x = self.bn4(self.bn4_2(self.rl4(self.tcn1(x))) + x2)\n",
    "        self.x = x\n",
    "        x = (self.rl5(self.tcn2(x))) \n",
    "        out = self.rl6(self.tcn3(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from src.processDepth import depthCompletion\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        root = r'C:\\Users\\dhruv\\Desktop\\680Final\\data\\0050'\n",
    "        self.depthFiles =  sorted(glob.glob(os.path.join(root, 'depth/*.png')))\n",
    "        self.depthFillFiles = sorted(glob.glob(os.path.join(root, 'depth/*.png')))\n",
    "\n",
    "        self.depthtransform = depthCompletion(15, None, None, None, None)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.depthFiles)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        img = np.float64(cv2.imread(self.depthFillFiles[idx], cv2.IMREAD_UNCHANGED))\n",
    "        goodDepth = self.depthtransform.fillDepth(img)\n",
    "        dp = transforms.ToTensor()(goodDepth)/1e3\n",
    "        tar = transforms.ToTensor()(img)/1e3\n",
    "        tar[tar>0.01] = 1\n",
    "        tar[tar<=0.01] = 0\n",
    "        return [dp, tar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torchvision\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "class train(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        loader = SimDataset()\n",
    "        self.train_loader = torch.utils.data.DataLoader(loader, shuffle=True, batch_size=32)\n",
    "        self.network = ruinDepth()\n",
    "        # self.tLoss = nn.MSELoss()\n",
    "        self.CELoss = nn.CrossEntropyLoss(weight=torch.Tensor([0.6,0.4]))\n",
    "\n",
    "        self.loss = 0\n",
    "        self.itr = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        pred = self.network.forward(images.float())        \n",
    "        # mask = targets==0\n",
    "        # loss =  0.5*self.tLoss(pred, targets.float()) + self.tLoss(pred[mask], targets[mask].float()) \n",
    "        loss = self.CELoss(pred, targets[:,0,:,:].long())\n",
    "\n",
    "        if batch_idx==0:\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            if self.current_epoch%10==0:\n",
    "                imgs = [i.unsqueeze(0).cpu().detach() for i in self.network.x[0]]\n",
    "                grid_img = utils.make_grid(imgs)  \n",
    "                self.logger.experiment.add_image('Inference/Skip Layer', grid_img, self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Input', images[0], self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Output', pred[0].unsqueeze(0), self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Target', targets[0], self.current_epoch)\n",
    "\n",
    "\n",
    "        self.loss+=loss; self.itr+=len(images)\n",
    "        return {'loss': loss}\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\n",
    "        self.log(\"loss\", self.loss/self.itr)\n",
    "        self.loss = 0\n",
    "        self.itr = 0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# TensorBoard Logger\n",
    "logger = pl.loggers.TensorBoardLogger('../logs/tb_logs', name='depthruiner')\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='epoch',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='my_model-{epoch:02d}-{loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Learning Rate Monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "model = train()\n",
    "\n",
    "# Trainer with logger, checkpoint, and LR monitor\n",
    "trainer = pl.Trainer(\n",
    "    devices=1, \n",
    "    accelerator=\"gpu\", \n",
    "    max_epochs=100, \n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, lr_monitor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# trainer = pl.Trainer(\n",
    "#     devices=1, \n",
    "#     accelerator=\"gpu\", \n",
    "#     max_epochs=100, \n",
    "#     logger=logger,\n",
    "#     callbacks=[checkpoint_callback, lr_monitor]\n",
    "# )\n",
    "\n",
    "model = model.load_from_checkpoint(r'C:\\Users\\dhruv\\Desktop\\680Final\\notebooks\\checkpoints\\my_model-epoch=41-loss=0.01.ckpt')\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotimg(img):\n",
    "#     plt.figure(); plt.imshow(img);plt.show()\n",
    "    \n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "# model.network.eval()\n",
    "# for img, tar in train_loader:\n",
    "#     synth = img[0][0].cpu().detach().numpy(); plotimg(synth)\n",
    "#     target = tar[0][0].cpu().detach().numpy(); plotimg(target)\n",
    "#     pred = model.network.forward(img.cuda()); pred = torch.argmax(pred, dim=1); pred = pred[0].cpu().detach().numpy()\n",
    "#     plotimg(pred)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthCompletion2:\n",
    "    \"\"\"\n",
    "    Unsuperivsed Depth completion\n",
    "    Reference: \n",
    "        J. Ku, et al., \"In Defense of Classical Image Processing: Fast Depth Completion on the CPU,\" 2018.\n",
    "    \"\"\"    \n",
    "    def __init__(self, maxDepth, custom_kernel, full_5, full_7, full_31):\n",
    "        self.max_depth = maxDepth\n",
    "        if custom_kernel is None:\n",
    "            #Replace this garbage with a function later\n",
    "            custom_kernel = np.asarray([[0, 0, 0, 1, 0, 0, 0],\n",
    "                                        [0, 0, 1, 1, 1, 0, 0],\n",
    "                                        [0, 1, 1, 1, 1, 1, 0],\n",
    "                                        [1, 1, 1, 1, 1, 1, 1],\n",
    "                                        [0, 1, 1, 1, 1, 1, 0],\n",
    "                                        [0, 0, 1, 1, 1, 0, 0],\n",
    "                                        [0, 0, 0, 1, 0, 0, 0],], dtype=np.uint8)\n",
    "\n",
    "        if full_5 is None:\n",
    "            full_5 = np.ones((5, 5), np.uint8)\n",
    "        if full_7 is None:\n",
    "            full_7 = np.ones((7, 7), np.uint8)\n",
    "        if full_31 is None:\n",
    "            full_31 = np.ones((31, 31), np.uint8)\n",
    "\n",
    "        self.custom_kernel = custom_kernel\n",
    "        self.full_kernel_5 = full_5\n",
    "        self.full_kernel_7 = full_7\n",
    "        self.full_kernel_31 = full_31\n",
    "\n",
    "    def fillDepth(self,inputImage):\n",
    "        #Convert the mm to meters to be consistent with the paper\n",
    "        img = np.float32(inputImage.copy())/1e3\n",
    "\n",
    "        self.max_depth = img.max()*1.2\n",
    "        #Threshold to maxDepth\n",
    "        # img[img>self.max_depth] = self.max_depth\n",
    "\n",
    "        #Valid image mask - 10 cm away\n",
    "        valid_depth_mask = img>1e-3\n",
    "\n",
    "        #1. Invert the Depth\n",
    "        img[valid_depth_mask] = self.max_depth - img[valid_depth_mask]\n",
    "\n",
    "        #2. Custom Kernel - ruins performance on inference :)\n",
    "        # img = cv2.dilate(img, self.custom_kernel)\n",
    "\n",
    "        #3. Small Hole Closure\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, self.full_kernel_5)\n",
    "        \n",
    "        #4. Small Hole Fill\n",
    "        invalid_mask = img<1e-3 #Remember depth is not changed in inversion\n",
    "        img[invalid_mask] = cv2.dilate(img, self.full_kernel_7)[invalid_mask]        \n",
    "        \n",
    "        #5. Large Hole Fill (authors loose patience at this step tbh)\n",
    "        invalid_mask = img<1e-3\n",
    "        img[invalid_mask] = cv2.dilate(img, self.full_kernel_31)[invalid_mask]\n",
    "\n",
    "        # #6. Median+Bilateral\n",
    "        img = cv2.medianBlur(img, 5)\n",
    "        img = cv2.bilateralFilter(img, 5, 1.5, 2.0) #Slow but better\n",
    "\n",
    "        #8 Depth correction\n",
    "        valid_depth_mask = img>1e-3\n",
    "        img[valid_depth_mask] = self.max_depth - img[valid_depth_mask]\n",
    "\n",
    "        return img*1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthB.png\", cv2.IMREAD_UNCHANGED)\n",
    "# dpt = np.float64(dpt)\n",
    "# dpt = transforms.ToTensor()(dpt)/1e3\n",
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "\n",
    "\n",
    "\n",
    "# # img2 = img.clone()[0][0]\n",
    "# img2 = dpt.clone()[0]\n",
    "# img2[mask] = img2.min()\n",
    "# plotimg(img2)\n",
    "# print(img2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.network.eval()\n",
    "# # with torch.no_grad():\n",
    "# #     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # # plotimg(pred_)\n",
    "# # # pred_ = pred.copy()\n",
    "# # dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# # mask = dp==0\n",
    "\n",
    "# from celluloid import Camera\n",
    "# fig, axs = plt.subplots(1, 2)\n",
    "# camera = Camera(fig)\n",
    "\n",
    "# root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "# depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "# model.network.eval()\n",
    "# # train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "# # model.network.eval()\n",
    "# # for img, tar in train_loader:\n",
    "# for i in depthFiles:\n",
    "#     dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "#     dpt = np.float64(dpt)\n",
    "#     dpt = transforms.ToTensor()(dpt)/1e3\n",
    "#     synth = dpt[0].cpu().detach().numpy().copy()\n",
    "#     # target = tar[0][0].cpu().detach().numpy();# plotimg(target)\n",
    "#     with torch.no_grad():\n",
    "#         pred_ = torch.argmax(model.network.forward(dpt.cuda().unsqueeze(0).float()),dim=1)[0].cpu().detach().numpy()\n",
    "#     dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((5,5))).fillDepth(pred_)\n",
    "#     mask = dp==0\n",
    "#     synth[mask] = synth.min()\n",
    "#     print(synth.min(),synth.max())\n",
    "#     axs[0].imshow(dpt[0].cpu().detach().numpy() * 1e3, cmap='gray')\n",
    "#     axs[0].axis('off')\n",
    "#     axs[0].set_title('Raw Image')\n",
    "\n",
    "#     axs[1].imshow(synth * 1e3, cmap='gray')\n",
    "#     axs[1].axis('off')\n",
    "#     axs[1].set_title('Predicted Image')\n",
    "\n",
    "#     # plt.show()\n",
    "#     camera.snap()\n",
    "# animation = camera.animate()\n",
    "# animation.save('animation.mp4', fps=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1916/1916 [01:28<00:00, 21.67it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "\n",
    "from celluloid import Camera\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "camera = Camera(fig)\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "model.network.eval()\n",
    "train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "from tqdm import tqdm\n",
    "for img, tar in tqdm(train_loader):\n",
    "    synth = img[0][0].cpu().detach().numpy().copy()\n",
    "    target = tar[0][0].cpu().detach().numpy();# plotimg(target)\n",
    "    with torch.no_grad():\n",
    "        pred_ = torch.argmax(model.network.forward(img.cuda().float()),dim=1)[0].cpu().detach().numpy()\n",
    "    dp = depthCompletion2(2, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,3))).fillDepth(pred_)\n",
    "    mask = dp==0\n",
    "    axs[0].imshow(synth)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Synthetic')\n",
    "\n",
    "    synth[mask] = 0\n",
    "    \n",
    "\n",
    "    axs[1].imshow(synth * 1e3)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Simulated')\n",
    "    synth = img[0][0].cpu().detach().numpy().copy()\n",
    "    mask = target==0\n",
    "    synth[mask] = 0\n",
    "\n",
    "\n",
    "    axs[2].imshow(synth)\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title('Real')\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "    camera.snap()\n",
    "    # break\n",
    "animation = camera.animate()\n",
    "animation.save('animation.mp4', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from celluloid import Camera\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2; import glob; import os\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "camera = Camera(fig)\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "rgbFiles =  sorted(glob.glob(os.path.join(root, '*rgbB.png')))[:30]\n",
    "\n",
    "for i,j in zip(depthFiles, rgbFiles):\n",
    "    dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "    # dpt = np.float64(dpt)\n",
    "    # dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)    \n",
    "    axs[0].imshow(dpt, cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Depth')\n",
    "    synth = cv2.imread(j, cv2.IMREAD_COLOR)\n",
    "\n",
    "    axs[1].imshow(synth)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('RGB')\n",
    "\n",
    "    # plt.show()\n",
    "    camera.snap()\n",
    "animation = camera.animate()\n",
    "animation.save('dataset.mp4', fps=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
