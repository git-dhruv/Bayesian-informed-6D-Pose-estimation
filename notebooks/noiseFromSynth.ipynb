{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  \n",
    "sys.path.append('../src')  \n",
    "\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# script_directory = os.path.dirname(os.path.realpath(__file__))\n",
    "# project_root = os.path.join(script_directory, '..')\n",
    "# sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthB.png\", cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(dpt)\n",
    "plt.figure()\n",
    "dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthA.png\", cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(dpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ruinDepth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 3, kernel_size=(3,3), padding=1)\n",
    "        self.rl1 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "\n",
    "        self.cn2 = nn.Conv2d(3, 7, kernel_size=(3,3), padding=1)\n",
    "        self.rl2 = nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(7)\n",
    "\n",
    "        self.cn3 = nn.Conv2d(7, 14, kernel_size=(3,3), padding=1)\n",
    "        self.rl3 = nn.LeakyReLU()\n",
    "        \n",
    "        self.tcn1 = nn.ConvTranspose2d(14, 7, kernel_size=3, padding=1)\n",
    "        self.rl4 = nn.Tanh()\n",
    "        self.bn4 = nn.BatchNorm2d(7)\n",
    "        self.bn4_2 = nn.BatchNorm2d(7)\n",
    "\n",
    "        self.tcn2 = nn.ConvTranspose2d(7, 3, kernel_size=3, stride=4, output_padding=1, padding=1)\n",
    "        self.rl5 = nn.Tanh()\n",
    "\n",
    "        self.tcn3 = nn.ConvTranspose2d(3, 2, kernel_size=3)\n",
    "        self.rl6 = nn.ReLU()\n",
    "\n",
    "        self.mpool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.mpool(self.bn1(self.rl1(self.cn1(x))))\n",
    "        x2 = self.mpool(self.bn2(self.rl2(self.cn2(x))))\n",
    "        x = (self.rl3(self.cn3(x2)))\n",
    "\n",
    "        x = self.bn4(self.bn4_2(self.rl4(self.tcn1(x))) + x2)\n",
    "        self.x = x\n",
    "        x = (self.rl5(self.tcn2(x))) \n",
    "        out = self.rl6(self.tcn3(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from src.processDepth import depthCompletion\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        root = r'C:\\Users\\dhruv\\Desktop\\680Final\\data\\0050'\n",
    "        self.depthFiles =  sorted(glob.glob(os.path.join(root, 'depth/*.png')))\n",
    "        self.depthFillFiles = sorted(glob.glob(os.path.join(root, 'depth/*.png')))\n",
    "\n",
    "        self.depthtransform = depthCompletion(15, None, None, None, None)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.depthFiles)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        img = np.float64(cv2.imread(self.depthFillFiles[idx], cv2.IMREAD_UNCHANGED))\n",
    "        goodDepth = self.depthtransform.fillDepth(img)\n",
    "        dp = transforms.ToTensor()(goodDepth)/1e3\n",
    "        tar = transforms.ToTensor()(img)/1e3\n",
    "        tar[tar>0.01] = 1\n",
    "        tar[tar<=0.01] = 0\n",
    "        return [dp, tar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torchvision\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "class train(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        loader = SimDataset()\n",
    "        self.train_loader = torch.utils.data.DataLoader(loader, shuffle=True, batch_size=32)\n",
    "        self.network = ruinDepth()\n",
    "        # self.tLoss = nn.MSELoss()\n",
    "        self.CELoss = nn.CrossEntropyLoss(weight=torch.Tensor([0.6,0.4]))\n",
    "\n",
    "        self.loss = 0\n",
    "        self.itr = 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        pred = self.network.forward(images.float())        \n",
    "        # mask = targets==0\n",
    "        # loss =  0.5*self.tLoss(pred, targets.float()) + self.tLoss(pred[mask], targets[mask].float()) \n",
    "        loss = self.CELoss(pred, targets[:,0,:,:].long())\n",
    "\n",
    "        if batch_idx==0:\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            if self.current_epoch%10==0:\n",
    "                imgs = [i.unsqueeze(0).cpu().detach() for i in self.network.x[0]]\n",
    "                grid_img = utils.make_grid(imgs)  \n",
    "                self.logger.experiment.add_image('Inference/Skip Layer', grid_img, self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Input', images[0], self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Output', pred[0].unsqueeze(0), self.current_epoch)\n",
    "                self.logger.experiment.add_image('Inference/Target', targets[0], self.current_epoch)\n",
    "\n",
    "\n",
    "        self.loss+=loss; self.itr+=len(images)\n",
    "        return {'loss': loss}\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\n",
    "        self.log(\"loss\", self.loss/self.itr)\n",
    "        self.loss = 0\n",
    "        self.itr = 0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# TensorBoard Logger\n",
    "logger = pl.loggers.TensorBoardLogger('../logs/tb_logs', name='depthruiner')\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='epoch',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='my_model-{epoch:02d}-{loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# Learning Rate Monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "model = train()\n",
    "\n",
    "# Trainer with logger, checkpoint, and LR monitor\n",
    "trainer = pl.Trainer(\n",
    "    devices=1, \n",
    "    accelerator=\"gpu\", \n",
    "    max_epochs=100, \n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, lr_monitor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# trainer = pl.Trainer(\n",
    "#     devices=1, \n",
    "#     accelerator=\"gpu\", \n",
    "#     max_epochs=100, \n",
    "#     logger=logger,\n",
    "#     callbacks=[checkpoint_callback, lr_monitor]\n",
    "# )\n",
    "\n",
    "model = model.load_from_checkpoint(r'C:\\Users\\dhruv\\Desktop\\680Final\\notebooks\\checkpoints\\my_model-epoch=41-loss=0.01.ckpt')\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotimg(img):\n",
    "#     plt.figure(); plt.imshow(img);plt.show()\n",
    "    \n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "# model.network.eval()\n",
    "# for img, tar in train_loader:\n",
    "#     synth = img[0][0].cpu().detach().numpy(); plotimg(synth)\n",
    "#     target = tar[0][0].cpu().detach().numpy(); plotimg(target)\n",
    "#     pred = model.network.forward(img.cuda()); pred = torch.argmax(pred, dim=1); pred = pred[0].cpu().detach().numpy()\n",
    "#     plotimg(pred)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthCompletion2:\n",
    "    \"\"\"\n",
    "    Unsuperivsed Depth completion\n",
    "    Reference: \n",
    "        J. Ku, et al., \"In Defense of Classical Image Processing: Fast Depth Completion on the CPU,\" 2018.\n",
    "    \"\"\"    \n",
    "    def __init__(self, maxDepth, custom_kernel, full_5, full_7, full_31):\n",
    "        self.max_depth = maxDepth\n",
    "        if custom_kernel is None:\n",
    "            #Replace this garbage with a function later\n",
    "            custom_kernel = np.asarray([[0, 0, 0, 1, 0, 0, 0],\n",
    "                                        [0, 0, 1, 1, 1, 0, 0],\n",
    "                                        [0, 1, 1, 1, 1, 1, 0],\n",
    "                                        [1, 1, 1, 1, 1, 1, 1],\n",
    "                                        [0, 1, 1, 1, 1, 1, 0],\n",
    "                                        [0, 0, 1, 1, 1, 0, 0],\n",
    "                                        [0, 0, 0, 1, 0, 0, 0],], dtype=np.uint8)\n",
    "\n",
    "        if full_5 is None:\n",
    "            full_5 = np.ones((5, 5), np.uint8)\n",
    "        if full_7 is None:\n",
    "            full_7 = np.ones((7, 7), np.uint8)\n",
    "        if full_31 is None:\n",
    "            full_31 = np.ones((31, 31), np.uint8)\n",
    "\n",
    "        self.custom_kernel = custom_kernel\n",
    "        self.full_kernel_5 = full_5\n",
    "        self.full_kernel_7 = full_7\n",
    "        self.full_kernel_31 = full_31\n",
    "\n",
    "    def fillDepth(self,inputImage):\n",
    "        #Convert the mm to meters to be consistent with the paper\n",
    "        img = np.float32(inputImage.copy())/1e3\n",
    "\n",
    "        self.max_depth = img.max()*1.2\n",
    "        #Threshold to maxDepth\n",
    "        # img[img>self.max_depth] = self.max_depth\n",
    "\n",
    "        #Valid image mask - 10 cm away\n",
    "        valid_depth_mask = img>1e-3\n",
    "\n",
    "        #1. Invert the Depth\n",
    "        img[valid_depth_mask] = self.max_depth - img[valid_depth_mask]\n",
    "\n",
    "        #2. Custom Kernel - ruins performance on inference :)\n",
    "        # img = cv2.dilate(img, self.custom_kernel)\n",
    "\n",
    "        #3. Small Hole Closure\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, self.full_kernel_5)\n",
    "        \n",
    "        #4. Small Hole Fill\n",
    "        invalid_mask = img<1e-3 #Remember depth is not changed in inversion\n",
    "        img[invalid_mask] = cv2.dilate(img, self.full_kernel_7)[invalid_mask]        \n",
    "        \n",
    "        #5. Large Hole Fill (authors loose patience at this step tbh)\n",
    "        invalid_mask = img<1e-3\n",
    "        img[invalid_mask] = cv2.dilate(img, self.full_kernel_31)[invalid_mask]\n",
    "\n",
    "        # #6. Median+Bilateral\n",
    "        img = cv2.medianBlur(img, 5)\n",
    "        img = cv2.bilateralFilter(img, 5, 1.5, 2.0) #Slow but better\n",
    "\n",
    "        #8 Depth correction\n",
    "        valid_depth_mask = img>1e-3\n",
    "        img[valid_depth_mask] = self.max_depth - img[valid_depth_mask]\n",
    "\n",
    "        return img*1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpt = cv2.imread(r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\\0000098depthB.png\", cv2.IMREAD_UNCHANGED)\n",
    "# dpt = np.float64(dpt)\n",
    "# dpt = transforms.ToTensor()(dpt)/1e3\n",
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "\n",
    "\n",
    "\n",
    "# # img2 = img.clone()[0][0]\n",
    "# img2 = dpt.clone()[0]\n",
    "# img2[mask] = img2.min()\n",
    "# plotimg(img2)\n",
    "# print(img2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.network.eval()\n",
    "# # with torch.no_grad():\n",
    "# #     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # # plotimg(pred_)\n",
    "# # # pred_ = pred.copy()\n",
    "# # dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# # mask = dp==0\n",
    "\n",
    "# from celluloid import Camera\n",
    "# fig, axs = plt.subplots(1, 2)\n",
    "# camera = Camera(fig)\n",
    "\n",
    "# root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "# depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "# model.network.eval()\n",
    "# # train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "# # model.network.eval()\n",
    "# # for img, tar in train_loader:\n",
    "# for i in depthFiles:\n",
    "#     dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "#     dpt = np.float64(dpt)\n",
    "#     dpt = transforms.ToTensor()(dpt)/1e3\n",
    "#     synth = dpt[0].cpu().detach().numpy().copy()\n",
    "#     # target = tar[0][0].cpu().detach().numpy();# plotimg(target)\n",
    "#     with torch.no_grad():\n",
    "#         pred_ = torch.argmax(model.network.forward(dpt.cuda().unsqueeze(0).float()),dim=1)[0].cpu().detach().numpy()\n",
    "#     dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((5,5))).fillDepth(pred_)\n",
    "#     mask = dp==0\n",
    "#     synth[mask] = synth.min()\n",
    "#     print(synth.min(),synth.max())\n",
    "#     axs[0].imshow(dpt[0].cpu().detach().numpy() * 1e3, cmap='gray')\n",
    "#     axs[0].axis('off')\n",
    "#     axs[0].set_title('Raw Image')\n",
    "\n",
    "#     axs[1].imshow(synth * 1e3, cmap='gray')\n",
    "#     axs[1].axis('off')\n",
    "#     axs[1].set_title('Predicted Image')\n",
    "\n",
    "#     # plt.show()\n",
    "#     camera.snap()\n",
    "# animation = camera.animate()\n",
    "# animation.save('animation.mp4', fps=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAC7CAYAAADovdO6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASLUlEQVR4nO3df0xV9ePH8RdeBQoSUCAMFRTyt1mptfwF/opZ+CvRnAtEzWE6zErNMoc/PtlQ549KXbmEldUMNcVGyx9hZmK5VSa6zFDMfqloMH+MDHh//2jcr9d7CTCFfPt8bK0499xzzqX79jzvueccvYwxRgAAALjpNajvDQAAAMD1QdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHY3QQKCwvl5eWlJUuW1Mn6YmNjFRsbWyfrwq0lMjJSycnJ9bLuuXPnysvLq17WXZXKsZ2ZmVnfmwLcELzH6x5hV4WDBw8qISFBERER8vX1VXh4uAYOHKjXXnvthq0zJydHc+fOvWHLv9Lhw4c1d+5cFRYW1sn6YL/6GDN1ZdWqVeyYcFPLzMyUl5eX85+GDRsqPDxcycnJ+uWXX+p783AdNazvDfgv2rt3r/r27auWLVtq4sSJCgsL08mTJ7Vv3z6tWLFCqampN2S9OTk5WrlyZZ3E3eHDhzVv3jzFxsYqMjLS5bFt27bd8PXDLjUdM0eOHFGDBjff58lVq1YpODi43o42AtfL/Pnz1apVK5WWlmrfvn3KzMzUnj17lJ+fL19f3/rePFwHhJ0HL7/8sgICArR//34FBga6PHb69On62ag65O3tXd+bgJtMTceMj49PHW8ZgCsNGjRI3bp1kyQ9+eSTCg4OVnp6urKzszVq1Kh63jpcDzffR+c6UFBQoI4dO7rtoCQpNDRUkhQTE6MuXbp4fH7btm0VFxcnyfX8uDfffFNRUVHy8fFR9+7dtX//fudzkpOTtXLlSklyOVx+tX9aRqXvv/9eCQkJatKkiXx9fdWtWzdlZ2c7H8/MzNTIkSMlSX379nWua9euXZI8n2NXWlqquXPnqk2bNvL19VWzZs302GOPqaCgoIrfIm4lNRkzkvs5dpVfD+3Zs0dTp05VSEiIAgMDlZKSosuXL6u4uFhJSUkKCgpSUFCQZs6cKWOM8/m7du1yee9Wqul5PRkZGerXr59CQ0Pl4+OjDh06aPXq1S7zREZG6tChQ/rss8+cY+XK8VFcXKxp06apRYsW8vHxUXR0tNLT01VRUeGynOLiYiUnJysgIECBgYEaO3asiouL/3H7gButd+/ekuTyZ3l1+xBJOnfunKZPn67OnTvL399fjRs31qBBg3TgwIE63X6444idBxEREcrLy1N+fr46derkcZ7ExERNnDjRbZ79+/frhx9+0EsvveQy/3vvvafz588rJSVFXl5eWrRokR577DEdO3ZMjRo1UkpKin799Vdt375d77zzjsd1VrcMSTp06JB69uyp8PBwzZo1S35+fvrggw80bNgwbdy4UcOHD1efPn00depUvfrqq3rxxRfVvn17SXL++2rl5eWKj4/Xzp07NXr0aD399NM6f/68tm/frvz8fEVFRdX6dwy71GTM/JPU1FSFhYVp3rx52rdvn958800FBgZq7969atmypRYuXKicnBwtXrxYnTp1UlJS0nXZ7tWrV6tjx44aMmSIGjZsqK1bt2ry5MmqqKjQlClTJEnLly9Xamqq/P39NXv2bEnSnXfeKUm6dOmSYmJi9MsvvyglJUUtW7bU3r179cILL+i3337T8uXLJUnGGA0dOlR79uzRpEmT1L59e3344YcaO3bsdXkdwLWqPM86KChIUs32IZJ07Ngxbd68WSNHjlSrVq106tQpvfHGG4qJidHhw4d111131ddLgoGbbdu2GYfDYRwOh3nooYfMzJkzzSeffGIuX77snKe4uNj4+vqa559/3uW5U6dONX5+fubChQvGGGOOHz9uJJmmTZuac+fOOefbsmWLkWS2bt3qnDZlyhTj6X9JbZbRv39/07lzZ1NaWuqcVlFRYXr06GHuvvtu57SsrCwjyeTm5rqtLyYmxsTExDh/Xrt2rZFkli5d6jZvRUWF2zTcemoyZowxJiIiwowdO9b5c0ZGhpFk4uLiXN5LDz30kPHy8jKTJk1yTisrKzPNmzd3eW/m5uZ6fB9XjpmMjAzntLS0NLfxdenSJbfXEhcXZ1q3bu0yrWPHji7rrbRgwQLj5+dnfvjhB5fps2bNMg6Hw/z000/GGGM2b95sJJlFixa5vJ7evXu7bSdwI1SOtR07dpgzZ86YkydPmg0bNpiQkBDj4+NjTp48aYyp+T6ktLTUlJeXu6zj+PHjxsfHx8yfP99lGu/xusVXsR4MHDhQeXl5GjJkiA4cOKBFixYpLi5O4eHhzsPRAQEBGjp0qN5//33nV0Pl5eVav369hg0bJj8/P5dlPv74485PRNL/H/4+duxYjberumWcO3dOn376qUaNGqXz58+rqKhIRUVFOnv2rOLi4nT06NFruvpp48aNCg4O9njRyH/t9hGoHzUZM/9kwoQJLu+lBx98UMYYTZgwwTnN4XCoW7dutRoz1bntttuc/11SUqKioiLFxMTo2LFjKikpqfb5WVlZ6t27t4KCgpzjraioSAMGDFB5ebl2794t6e8Loxo2bKinnnrK5fXcqAuxgKoMGDBAISEhatGihRISEuTn56fs7Gw1b968VvsQHx8f54VQ5eXlOnv2rPz9/dW2bVt9/fXX9fkSb3l8FVuF7t27a9OmTbp8+bIOHDigDz/8UMuWLVNCQoK+/fZbdejQQUlJSVq/fr0+//xz9enTRzt27NCpU6eUmJjotryWLVu6/FwZaH/88UeNt6m6Zfz4448yxmjOnDmaM2eOx2WcPn1a4eHhNV6n9Pe5F23btlXDhrxdULWajJmqXP3eDggIkCS1aNHCbXptxkx1vvjiC6WlpSkvL0+XLl1yeaykpMS5HVU5evSovvvuO4WEhHh8vPLCkRMnTqhZs2by9/d3ebxt27b/YuuB2lu5cqXatGmjkpISrV27Vrt373Ze1FSbfUhFRYVWrFihVatW6fjx4yovL3fO07Rp0zp5LfCMPXU1vL291b17d3Xv3l1t2rTRuHHjlJWVpbS0NMXFxenOO+/UunXr1KdPH61bt05hYWEaMGCA23IcDofH5ZsrTgSvTnXLqDxZe/r06c6LN64WHR1d4/UB1+KfxkxVqnpve5p+5Zip6ojxlTuZqhQUFKh///5q166dli5dqhYtWsjb21s5OTlatmyZ28UPnlRUVGjgwIGaOXOmx8fbtGlT7TKAuvTAAw84r4odNmyYevXqpTFjxujIkSO12ocsXLhQc+bM0fjx47VgwQI1adJEDRo00LRp02o0dnDjEHa1UDkYfvvtN0l/73TGjBmjzMxMpaena/PmzZo4cWKVO6nq/NuvNVu3bi1JatSokce4vNZ1RUVF6csvv9Rff/3lvEgDqImrx8z1VnnU+uqrS0+cOFHtc7du3ao///xT2dnZLkcMc3Nz3eatarxERUXpwoUL1Y63iIgI7dy5UxcuXHA5anfkyJFqtxO4URwOh1555RX17dtXr7/+usaPHy+pZvuQDRs2qG/fvnrrrbdcphcXFys4OPiGbTOqxzl2HuTm5no8kpaTkyPJ9euTxMRE/fHHH0pJSdGFCxf0xBNPXPN6K8/Lu9ZbIISGhio2NlZvvPGGxx3pmTNnrmldI0aMUFFRkV5//XW3x2pzxBH2qs2YuZ4iIiLkcDic57JVWrVqVbXPrfwAduV2l5SUKCMjw21ePz8/j2Nl1KhRysvL0yeffOL2WHFxscrKyiRJjzzyiMrKylxupVJeXm7F38qBm1tsbKweeOABLV++XI0bN67xPsThcLiN+aysLP4Wi/8Ajth5kJqaqkuXLmn48OFq166dLl++rL1792r9+vWKjIzUuHHjnPPed9996tSpk7KystS+fXvdf//917zerl27SpKmTp2quLg4ORwOjR49ulbLWLlypXr16qXOnTtr4sSJat26tU6dOqW8vDz9/PPPznsM3XvvvXI4HEpPT1dJSYl8fHyc9/O6WlJSkt5++209++yz+uqrr9S7d29dvHhRO3bs0OTJkzV06NBrfs2wQ23GzPUUEBCgkSNH6rXXXpOXl5eioqL00Ucf1ehG4g8//LC8vb01ePBg5wezNWvWKDQ01G2n1rVrV61evVr/+9//FB0drdDQUPXr108zZsxQdna24uPjlZycrK5du+rixYs6ePCgNmzYoMLCQgUHB2vw4MHq2bOnZs2apcLCQnXo0EGbNm2q0QUawI02Y8YMjRw5UpmZmTXeh8THx2v+/PkaN26cevTooYMHD+rdd991fnOEelQ/F+P+t3388cdm/Pjxpl27dsbf3994e3ub6Ohok5qaak6dOuU2/6JFi4wks3DhQrfHKi/1Xrx4sdtjkkxaWprz57KyMpOammpCQkKMl5eX89YMtVmGMcYUFBSYpKQkExYWZho1amTCw8NNfHy82bBhg8t8a9asMa1btzYOh8PllhFX3+7EmL9vCzF79mzTqlUr06hRIxMWFmYSEhJMQUGBp18hbjE1HTNV3e5k//79LsurvDXJmTNnXKaPHTvW+Pn5uUw7c+aMGTFihLn99ttNUFCQSUlJMfn5+TW63Ul2dra55557jK+vr4mMjDTp6enO2/scP37cOd/vv/9uHn30UXPHHXcYSS7j4/z58+aFF14w0dHRxtvb2wQHB5sePXqYJUuWuNzu5ezZsyYxMdE0btzYBAQEmMTERPPNN99wKwjUiarGmjHGlJeXm6ioKBMVFWXKyspqtA8pLS01zz33nGnWrJm57bbbTM+ePU1eXp7b/oPbndQ9L2P4Lu3fWrFihZ555hkVFha6Xd0HAABQVwi7f8kYoy5duqhp06YeT7oGAACoK5xjd40uXryo7Oxs5ebm6uDBg9qyZUt9bxIAALjFccTuGhUWFqpVq1YKDAzU5MmT9fLLL9f3JgEAgFscYQcAAGAJ7mMHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACxB2AEAAFiCsAMAALAEYQcAAGAJwg4AAMAShB0AAIAlCDsAAABLEHYAAACWIOwAAAAsQdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACxB2AEAAFiCsAMAALAEYQcAAGAJwg4AAMAShB0AAIAlCDsAAABLEHYAAACWIOwAAAAsQdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACxB2AEAAFiCsAMAALAEYQcAAGAJwg4AAMAShB0AAIAlCDsAAABLEHYAAACWIOwAAAAsQdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACxB2AEAAFiCsAMAALAEYQcAAGAJwg4AAMAShB0AAIAlCDsAAABLEHYAAACWIOwAAAAsQdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACxB2AEAAFiCsAMAALAEYQcAAGAJwg4AAMAShB0AAIAlCDsAAABLEHYAAACWIOwAAAAsQdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACxB2AEAAFiCsAMAALAEYQcAAGAJwg4AAMAShB0AAIAlCDsAAABLEHYAAACWIOwAAAAsQdgBAABYgrADAACwBGEHAABgCcIOAADAEoQdAACAJQg7AAAASxB2AAAAliDsAAAALEHYAQAAWIKwAwAAsARhBwAAYAnCDgAAwBKEHQAAgCUIOwAAAEsQdgAAAJYg7AAAACzxfzc/ebwqzzFAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "\n",
    "from celluloid import Camera\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "camera = Camera(fig)\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "model.network.eval()\n",
    "train_loader = torch.utils.data.DataLoader(SimDataset(), shuffle=False, batch_size=1)\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sz = (640*1,480*1)\n",
    "\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    # Get memory information\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Print memory information\n",
    "    print(f\"Total Memory: {memory_info.total} bytes\")\n",
    "    print(f\"Available Memory: {memory_info.available} bytes\")\n",
    "    print(f\"Used Memory: {memory_info.used} bytes\")\n",
    "    print(f\"Memory Percent: {memory_info.percent}%\")\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for img, tar in tqdm(train_loader):\n",
    "    synth = img[0][0].cpu().detach().numpy().copy()\n",
    "    target = tar[0][0].cpu().detach().numpy();# plotimg(target)\n",
    "    with torch.no_grad():\n",
    "        pred_ = torch.argmax(model.network.forward(img.cuda().float()),dim=1)[0].cpu().detach().numpy()\n",
    "    dp = depthCompletion2(2, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "    mask = dp==0\n",
    "    axs[0].imshow(cv2.resize(synth, dsize=sz)  )\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Synthetic')\n",
    "\n",
    "    synth[mask] = 0\n",
    "    \n",
    "\n",
    "    axs[1].imshow(cv2.resize(synth * 1e3, dsize=sz))\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Simulated')\n",
    "    synth = img[0][0].cpu().detach().numpy().copy()\n",
    "    mask = target==0\n",
    "    synth[mask] = 0\n",
    "    axs[2].imshow(cv2.resize(synth, dsize=sz))\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title('Real')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "    i+=1\n",
    "    camera.snap()\n",
    "    if i%10==0:\n",
    "        print_memory_usage()\n",
    "animation = camera.animate()\n",
    "animation.save('animation.mp4', fps=60, dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del animation\n",
    "del camera\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from celluloid import Camera\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2; import glob; import os\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "camera = Camera(fig)\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\validation_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))[:30]\n",
    "rgbFiles =  sorted(glob.glob(os.path.join(root, '*rgbB.png')))[:30]\n",
    "\n",
    "for i,j in zip(depthFiles, rgbFiles):\n",
    "    dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "    # dpt = np.float64(dpt)\n",
    "    # dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)    \n",
    "    axs[0].imshow(dpt, cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Depth')\n",
    "    synth = cv2.imread(j, cv2.IMREAD_COLOR)\n",
    "\n",
    "    axs[1].imshow(synth)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('RGB')\n",
    "\n",
    "    # plt.show()\n",
    "    camera.snap()\n",
    "animation = camera.animate()\n",
    "animation.save('dataset.mp4', fps=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.network.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred_ = torch.argmax(model.network.forward(dpt.float().cuda().unsqueeze(0)),dim=1)[0].cpu().detach().numpy()\n",
    "# # plotimg(pred_)\n",
    "# # pred_ = pred.copy()\n",
    "# dp = depthCompletion2(15, None, np.ones((3,3)), np.ones((2,6)), np.ones((1,1))).fillDepth(pred_)\n",
    "# mask = dp==0\n",
    "import glob, os, tqdm\n",
    "\n",
    "root = r\"C:\\Users\\dhruv\\Desktop\\680Final\\data\\mustard_bottle\\train_data_blender_DR\"\n",
    "depthFiles =  sorted(glob.glob(os.path.join(root, '*depthB.png')))\n",
    "model.network.eval()\n",
    "for i in tqdm.tqdm(depthFiles):\n",
    "    dpt = cv2.imread(i, cv2.IMREAD_UNCHANGED)\n",
    "    dpt = np.float64(dpt)\n",
    "    dpt = transforms.ToTensor()(dpt)/1e3\n",
    "    synth = dpt[0].cpu().detach().numpy().copy()\n",
    "    with torch.no_grad():\n",
    "        pred_ = torch.argmax(model.network.forward(dpt.cuda().unsqueeze(0).float()),dim=1)[0].cpu().detach().numpy()\n",
    "    dp = depthCompletion2(5, None, np.ones((3,3)), np.ones((2,6)), np.ones((5,5))).fillDepth(pred_)\n",
    "    mask = dp==0\n",
    "    synth[mask] = 0\n",
    "    synth = synth * 1e3\n",
    "    synth = synth.astype(np.uint16)\n",
    "    file = i.split(\".png\")[0]+\"_fake.png\"\n",
    "    cv2.imwrite( file, synth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.float64(cv2.imread(i, cv2.IMREAD_UNCHANGED))\n",
    "d2 = np.float64(cv2.imread(file, cv2.IMREAD_UNCHANGED))\n",
    "print(d1.min())\n",
    "plt.imshow(d1)\n",
    "plt.figure()\n",
    "d2[d2==0] = 625\n",
    "plt.imshow(d2)\n",
    "print(file)\n",
    "# plt.imshow(d2!=d1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
